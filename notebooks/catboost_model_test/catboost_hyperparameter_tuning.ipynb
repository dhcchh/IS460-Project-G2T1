{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CatBoost Hyperparameter Tuning for Fraud Detection\n",
        "\n",
        "This notebook performs grid search hyperparameter tuning for the CatBoost model using the `CatBoostGridSearchTuner` class.\n",
        "\n",
        "## Objectives\n",
        "1. Define parameter grid for tuning\n",
        "2. Train multiple CatBoost configurations\n",
        "3. Compare performance across hyperparameters\n",
        "4. Identify best configuration\n",
        "5. Compare best CatBoost against baseline models\n",
        "\n",
        "## Hyperparameters to Tune\n",
        "- **Learning Rate**: Gradient boosting step size\n",
        "- **Depth**: Tree depth\n",
        "- **L2 Leaf Regularization**: Overfitting control\n",
        "- **Iterations**: Number of trees\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment ready for CatBoost tuning.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.abspath('../..'))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "\n",
        "from src.catboost_models import CatBoostGridSearchTuner\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Environment ready for CatBoost hyperparameter tuning.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Define Base Configuration and Parameter Grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparameter Tuning Configuration\n",
            "============================================================\n",
            "Total Configurations: 18\n",
            "  iterations     : [200, 400]\n",
            "  depth          : [4, 6, 8]\n",
            "  learning_rate  : [0.1, 0.05, 0.02]\n"
          ]
        }
      ],
      "source": [
        "# Base configuration (parameters that won't change)\n",
        "base_config = {\n",
        "    # Data\n",
        "    'data_path': '../../data/processed/creditcard_fe.csv',\n",
        "    'drop_features': 'logreg_baseline',\n",
        "    \n",
        "    # Fixed training parameters\n",
        "    'l2_leaf_reg': 3.0,  # Default regularization\n",
        "    \n",
        "    # Business costs\n",
        "    'C_FP': 550,  # False positive cost (investigation)\n",
        "    'C_FN': 110,  # False negative cost (missed fraud)\n",
        "    \n",
        "    # Hardware\n",
        "    'random_seed': 42,\n",
        "    \n",
        "    # Save directories\n",
        "    'model_save_dir': '../../models/',\n",
        "}\n",
        "\n",
        "# Parameter grid to search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.1, 0.05, 0.02],   # Gradient step size\n",
        "    'depth': [4, 6, 8],                    # Tree depth\n",
        "    'l2_leaf_reg': [1.0, 3.0, 5.0],       # Regularization\n",
        "    'iterations': [200, 400]              # Number of trees\n",
        "}\n",
        "\n",
        "# Calculate total configurations\n",
        "total_configs = np.prod([len(v) for v in param_grid.values()])\n",
        "\n",
        "print(\"Hyperparameter Tuning Configuration\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBase Configuration:\")\n",
        "for key, value in base_config.items():\n",
        "    print(f\"  {key:20s}: {value}\")\n",
        "\n",
        "print(f\"\\nParameter Grid:\")\n",
        "for key, values in param_grid.items():\n",
        "    print(f\"  {key:20s}: {values}\")\n",
        "\n",
        "print(f\"\\nTotal Configurations: {total_configs}\")\n",
        "print(f\"Estimated Time (1 min/config): ~{total_configs} minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Run Grid Search\n",
        "\n",
        "This will train all configurations and track results. The best model (by validation cost) will be automatically saved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from ../../data/processed/creditcard_fe.csv...\n",
            "Dataset loaded: 284807 transactions\n",
            "  Normal: 284315 (99.83%)\n",
            "  Fraud: 492 (0.17%)\n",
            "  Features: 22\n",
            "  Dropped features: 14\n",
            "\n",
            "Data split:\n",
            "  Training: 170589 normal transactions\n",
            "  Validation: 57109 transactions (246 fraud)\n",
            "  Test: 57109 transactions (246 fraud)\n",
            "Data ready.\n"
          ]
        }
      ],
      "source": [
        "results_csv_path = '../../results/tuning/catboost_grid_search_results.csv'\n",
        "\n",
        "if os.path.exists(results_csv_path):\n",
        "    print(\"=\" * 70)\n",
        "    print(\"LOADING PRE-COMPUTED GRID SEARCH RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    results_df = pd.read_csv(results_csv_path)\n",
        "    \n",
        "    print(f\"\\n✓ Successfully loaded {len(results_df)} configurations\")\n",
        "    print(f\"✓ Best validation cost: ${results_df['val_cost'].min():,.0f}\")\n",
        "    print(f\"✓ Best test cost: ${results_df['test_cost'].min():,.0f}\")\n",
        "    \n",
        "    # Show best configuration\n",
        "    best_idx = results_df['val_cost'].idxmin()\n",
        "    best_config = results_df.loc[best_idx]\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"BEST CONFIGURATION (by validation cost):\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Config Index:     {int(best_config['config_idx'])}\")\n",
        "    print(f\"  Learning Rate:    {best_config['learning_rate']}\")\n",
        "    print(f\"  Depth:           {int(best_config['depth'])}\")\n",
        "    print(f\"  L2 Leaf Reg:     {best_config['l2_leaf_reg']}\")\n",
        "    print(f\"  Iterations:       {int(best_config['iterations'])}\")\n",
        "    print(f\"\\n  Validation Cost:  ${int(best_config['val_cost']):,}\")\n",
        "    print(f\"  Test Cost:        ${int(best_config['test_cost']):,}\")\n",
        "    print(f\"  Precision:        {best_config['test_precision']:.3f}\")\n",
        "    print(f\"  Recall:           {best_config['test_recall']:.3f}\")\n",
        "    print(f\"  PR-AUC:           {best_config['test_pr_auc']:.4f}\")\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"✓ Ready to analyze! Continue to Section 3\")\n",
        "    print(f\"{'='*70}\")\n",
        "else:\n",
        "    print(f\"Results not found at: {results_csv_path}\")\n",
        "    print(\"Please run the grid search cell below to generate results.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Run Full Grid Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completed 18 configurations\n",
            "Best by validation cost:\n",
            "iterations          400.000000\n",
            "depth                 8.000000\n",
            "learning_rate         0.100000\n",
            "val_cost          22000.000000\n",
            "threshold             0.008046\n",
            "test_cost         25740.000000\n",
            "test_precision        0.841463\n",
            "test_recall           0.841463\n",
            "test_pr_auc           0.861649\n",
            "Name: 15, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Uncomment to run full grid search\n",
        "# tuner = CatBoostGridSearchTuner(\n",
        "#     base_config=base_config,\n",
        "#     param_grid=param_grid,\n",
        "#     results_dir='../../results/tuning/'\n",
        "# )\n",
        "# \n",
        "# results_df = tuner.run_grid_search()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Analyze Tuning Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: results/tuning/catboost_grid_search_results.csv\n",
            "Saved: results/tuning/catboost_grid_search_detailed.json\n"
          ]
        }
      ],
      "source": [
        "# Display top 10 configurations by test cost\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TOP 10 CONFIGURATIONS BY TEST COST\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "top10 = results_df.nsmallest(10, 'test_cost')[[\n",
        "    'config_idx', 'learning_rate', 'depth', 'l2_leaf_reg', 'iterations',\n",
        "    'test_cost', 'test_precision', 'test_recall', 'test_pr_auc'\n",
        "]].copy()\n",
        "\n",
        "# Format for display\n",
        "top10['test_cost'] = top10['test_cost'].apply(lambda x: f\"${int(x):,}\")\n",
        "top10['test_precision'] = top10['test_precision'].apply(lambda x: f\"{x:.3f}\")\n",
        "top10['test_recall'] = top10['test_recall'].apply(lambda x: f\"{x:.3f}\")\n",
        "top10['test_pr_auc'] = top10['test_pr_auc'].apply(lambda x: f\"{x:.4f}\")\n",
        "\n",
        "print(top10.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TUNING SUMMARY STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nTest Cost:\")\n",
        "print(f\"  Best: ${results_df['test_cost'].min():,.0f}\")\n",
        "print(f\"  Worst: ${results_df['test_cost'].max():,.0f}\")\n",
        "print(f\"  Mean: ${results_df['test_cost'].mean():,.0f}\")\n",
        "print(f\"  Std Dev: ${results_df['test_cost'].std():,.0f}\")\n",
        "\n",
        "print(f\"\\nPrecision:\")\n",
        "print(f\"  Best: {results_df['test_precision'].max():.3f}\")\n",
        "print(f\"  Worst: {results_df['test_precision'].min():.3f}\")\n",
        "print(f\"  Mean: {results_df['test_precision'].mean():.3f}\")\n",
        "\n",
        "print(f\"\\nRecall:\")\n",
        "print(f\"  Best: {results_df['test_recall'].max():.3f}\")\n",
        "print(f\"  Worst: {results_df['test_recall'].min():.3f}\")\n",
        "print(f\"  Mean: {results_df['test_recall'].mean():.3f}\")\n",
        "\n",
        "print(f\"\\nPR-AUC:\")\n",
        "print(f\"  Best: {results_df['test_pr_auc'].max():.4f}\")\n",
        "print(f\"  Worst: {results_df['test_pr_auc'].min():.4f}\")\n",
        "print(f\"  Mean: {results_df['test_pr_auc'].mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Impact of Individual Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze impact of each hyperparameter on test cost\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Learning Rate\n",
        "ax = axes[0, 0]\n",
        "grouped = results_df.groupby('learning_rate')['test_cost'].agg(['mean', 'std', 'min', 'max'])\n",
        "ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'], \n",
        "            marker='o', capsize=5, linewidth=2, markersize=8)\n",
        "ax.set_xlabel('Learning Rate', fontsize=12)\n",
        "ax.set_ylabel('Test Cost ($)', fontsize=12)\n",
        "ax.set_title('Impact of Learning Rate on Cost', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Depth\n",
        "ax = axes[0, 1]\n",
        "grouped = results_df.groupby('depth')['test_cost'].agg(['mean', 'std', 'min', 'max'])\n",
        "ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'], \n",
        "            marker='o', capsize=5, linewidth=2, markersize=8, color='coral')\n",
        "ax.set_xlabel('Depth', fontsize=12)\n",
        "ax.set_ylabel('Test Cost ($)', fontsize=12)\n",
        "ax.set_title('Impact of Depth on Cost', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. L2 Leaf Regularization\n",
        "ax = axes[1, 0]\n",
        "grouped = results_df.groupby('l2_leaf_reg')['test_cost'].agg(['mean', 'std', 'min', 'max'])\n",
        "ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'], \n",
        "            marker='o', capsize=5, linewidth=2, markersize=8, color='green')\n",
        "ax.set_xlabel('L2 Leaf Regularization', fontsize=12)\n",
        "ax.set_ylabel('Test Cost ($)', fontsize=12)\n",
        "ax.set_title('Impact of L2 Regularization on Cost', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Iterations\n",
        "ax = axes[1, 1]\n",
        "grouped = results_df.groupby('iterations')['test_cost'].agg(['mean', 'std', 'min', 'max'])\n",
        "ax.errorbar(grouped.index, grouped['mean'], yerr=grouped['std'], \n",
        "            marker='o', capsize=5, linewidth=2, markersize=8, color='purple')\n",
        "ax.set_xlabel('Iterations', fontsize=12)\n",
        "ax.set_ylabel('Test Cost ($)', fontsize=12)\n",
        "ax.set_title('Impact of Iterations on Cost', fontsize=13, fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../../results/figures/catboost_hyperparameter_impact_analysis.png', \n",
        "            dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nHyperparameter impact plot saved: results/figures/catboost_hyperparameter_impact_analysis.png\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Dev-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
